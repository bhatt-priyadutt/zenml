{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RO_v5iIaYFi2"
   },
   "source": [
    "# ZenML Quickstart Guide\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/notebooks/quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spin up quickstart stack using sandbox\n",
    "2. Define custom dataset to train on\n",
    "3. Train gpt-nano locally on CPU and deploy with mlflow/bentoml\n",
    "4. Show run in dashboard\n",
    "5. Spin up gradio app to interact with it -> still sucks\n",
    "6. Define remote stack with cloud GPU step operator and Seldon deployer\n",
    "7. Show stack in dashboard\n",
    "8. Train gpt-xl on remote stack\n",
    "9. Show run in dashboard\n",
    "10. Run gradio with app with new model -> much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/karpathy/minGPT.git\n",
    "!pip install -e minGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install pytorch mlflow -y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to ZenML Sandbox\n",
    "## TODO: Change stack definitions below using sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the MLflow experiment tracker\n",
    "!zenml experiment-tracker register mlflow_tracker --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model registry\n",
    "!zenml model-registry register mlflow_registry --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model deployer\n",
    "!zenml model-deployer register mlflow_deployer --flavor=mlflow\n",
    "\n",
    "# Register a new stack with the new stack components\n",
    "!zenml stack register quickstart_stack -a default\\\n",
    "                                       -o default\\\n",
    "                                       -d mlflow_deployer\\\n",
    "                                       -e mlflow_tracker\\\n",
    "                                       -r mlflow_registry\\\n",
    "\n",
    "!zenml stack set quickstart_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from unstructured.partition.html import partition_html\n",
    "from zenml.steps import step\n",
    "from mingpt.bpe import BPETokenizer\n",
    "\n",
    "\n",
    "class UrlTokenDataset(Dataset):\n",
    "    def __init__(self, url, block_size=20):\n",
    "        self.block_size = block_size\n",
    "        elements = partition_html(url=url)\n",
    "        text = \"\\n\\n\".join([str(el) for el in elements])\n",
    "        tokenizer = BPETokenizer()\n",
    "        tokens = tokenizer(text).flatten()\n",
    "        self.examples = []\n",
    "        for i in range(0, len(tokens) - block_size + 1, block_size):\n",
    "            self.examples.append(tokens[i:i + block_size + 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        example = self.examples[item]\n",
    "        x = example[:-1].clone()\n",
    "        y = example[1:].clone()\n",
    "        return x, y\n",
    "\n",
    "@step\n",
    "def load_url_dataset() -> UrlTokenDataset:\n",
    "    \"\"\"Data loader step.\"\"\"\n",
    "    return UrlTokenDataset(\"https://zenml.io\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train gpt-nano with mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.model import GPT\n",
    "from zenml.steps import step\n",
    "\n",
    "@step\n",
    "def load_gpt_nano() -> GPT:\n",
    "    model_config = GPT.get_default_config()\n",
    "    model_config.model_type = 'gpt-nano'\n",
    "    model_config.vocab_size = 50257 # openai's model vocabulary\n",
    "    model_config.block_size = 1024  # openai's model block_size (i.e. input context length)\n",
    "    model = GPT(model_config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "from zenml.client import Client\n",
    "import mlflow\n",
    "\n",
    "experiment_tracker = Client().active_stack.experiment_tracker\n",
    "\n",
    "@step(experiment_tracker=experiment_tracker.name)\n",
    "def train_llm(dataset: Dataset, model: Module) -> Module:\n",
    "    from mingpt.trainer import Trainer\n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.learning_rate = 5e-4\n",
    "    train_config.max_iters = 100  # TODO: 2000; make this configurable\n",
    "    train_config.num_workers = 0\n",
    "    train_config.device = 'mps'  # TODO: make this configurable\n",
    "    trainer = Trainer(train_config, model, dataset)\n",
    "    def batch_end_callback(trainer):\n",
    "        if trainer.iter_num % 100 == 0:\n",
    "            print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "    trainer.run()\n",
    "    model.__call__ = lambda *args, **kwargs: model.forward(*args, **kwargs)[0]\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Delete step below once new pipeline definition is life and simply set decision=True in deployer step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def deployment_trigger() -> bool:\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Switch to new pipeline definition style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "\n",
    "@pipeline(enable_cache=False)\n",
    "def training_pipeline(\n",
    "    load_training_data,\n",
    "    model_definition,\n",
    "    trainer,\n",
    "    deployment_trigger,\n",
    "    model_deployer,\n",
    "):\n",
    "    \"\"\"Train, evaluate, and deploy a model.\"\"\"\n",
    "    dataset = load_training_data()\n",
    "    model = model_definition()\n",
    "    model = trainer(dataset, model)\n",
    "    deployment_decision = deployment_trigger()\n",
    "    model_deployer(deployment_decision, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\u001b[1;35mReusing registered pipeline \u001b[0m\u001b[33mtraining_pipeline\u001b[1;35m (version: 9).\u001b[0m\n",
      "\u001b[1;35mRunning pipeline \u001b[0m\u001b[33mtraining_pipeline\u001b[1;35m on stack \u001b[0m\u001b[33mquickstart_stack\u001b[1;35m (caching disabled)\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mdeployment_trigger\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mdeployment_trigger\u001b[1;35m has finished in 0.027s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmodel_definition\u001b[1;35m has started.\u001b[0m\n",
      "number of parameters: 2.55M\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmodel_definition\u001b[1;35m has finished in 0.166s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mload_training_data\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unstructured:Reading document from string ...\n",
      "INFO:unstructured:Reading document ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStep \u001b[0m\u001b[33mload_training_data\u001b[1;35m has finished in 0.356s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mtrainer\u001b[1;35m has started.\u001b[0m\n",
      "running on device mps\n",
      "iter_dt 0.00ms; iter 0: train loss 10.83128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/03 12:21:55 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-02-28; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'mingpt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStep \u001b[0m\u001b[33mtrainer\u001b[1;35m has finished in 12.549s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmodel_deployer\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mUpdating an existing MLflow deployment service: MLFlowDeploymentService[dc85f323-6eb1-42b5-85e6-a9cb9da9c1cd] (type: model-serving, flavor: mlflow)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc2334610a5404582603dd3fbc8a015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h\r\u001b[1A\u001b[2K\u001b[1A\u001b[2K"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mMLflow deployment service started and reachable at:\n",
      "    http://127.0.0.1:8004/invocations\n",
      "\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmodel_deployer\u001b[1;35m has finished in 14.766s.\u001b[0m\n",
      "\u001b[1;35mPipeline run \u001b[0m\u001b[33mtraining_pipeline-2023_05_03-10_21_42_603868\u001b[1;35m has finished in 28.298s.\u001b[0m\n",
      "\u001b[1;35mPipeline visualization can be seen in the ZenML Dashboard. Run \u001b[0m\u001b[33mzenml up\u001b[1;35m to see your pipeline!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from zenml.integrations.mlflow.steps import mlflow_model_deployer_step\n",
    "\n",
    "pip = training_pipeline(\n",
    "    load_training_data=load_url_dataset(),\n",
    "    model_definition=load_gpt_nano(),\n",
    "    trainer=train_llm(),\n",
    "    deployment_trigger=deployment_trigger(),\n",
    "    model_deployer=mlflow_model_deployer_step(),\n",
    ")\n",
    "pip.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36mUsing the default local database.\u001b[0m\n",
      "\u001b[2;36mRunning with active workspace: \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "\u001b[2;36mRunning with active stack: \u001b[0m\u001b[2;32m'quickstart_stack'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━┓\n",
      "┃\u001b[1m        \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m \u001b[0m\u001b[1mPIPELINE_STEP_NA\u001b[0m\u001b[1m \u001b[0m│\u001b[1m            \u001b[0m┃\n",
      "┃\u001b[1m \u001b[0m\u001b[1mSTATUS\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mUUID            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mPIPELINE_NAME   \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mME              \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mMODEL_NAME\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┠────────┼──────────────────┼──────────────────┼──────────────────┼────────────┨\n",
      "┃   ✅   │ dc85f323-6eb1-42 │ training_pipelin │ model_deployer   │ model      ┃\n",
      "┃        │ b5-85e6-a9cb9da9 │ e                │                  │            ┃\n",
      "┃        │ c1cd             │                  │                  │            ┃\n",
      "┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━┛\n"
     ]
    }
   ],
   "source": [
    "!zenml model-deployer models list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Show run in dashboard TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spin up gradio app\n",
    "\n",
    "### TODO: use deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.post_execution import get_run\n",
    "\n",
    "pipeline_run = pip.get_runs()[0]\n",
    "deployer_step = pipeline_run.get_step(\"model_deployer\")\n",
    "deployed_model_url = deployer_step.metadata[\"deployed_model_url\"].value\n",
    "deployed_model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "\n",
    "tokenizer(\"Zenml is\").numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_gpt_nano().entrypoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Zenml is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "    \"instances\": [\n",
    "        tokenizer(\"Zenml is\").flatten().numpy().tolist(),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "response = requests.post(\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    url=deployed_model_url,\n",
    "    data=json.dumps(data)\n",
    "    # json=\"[[47573,  4029,   318]]\",\n",
    "    # json=tokenizer(\"Zenml is\").flatten().numpy().tolist()\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.client import Client\n",
    "\n",
    "client = Client()\n",
    "model_deployer = client.active_stack.model_deployer\n",
    "services = model_deployer.find_model_server(\n",
    "    pipeline_name=pip.name,\n",
    "    pipeline_step_name=\"model_deployer\",\n",
    "    running=True,\n",
    ")\n",
    "service = services[0]\n",
    "service.check_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction = service.predict(np.array([[0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.bpe import BPETokenizer\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "def generate(prompt='', steps=20):\n",
    "    tokenizer = BPETokenizer()\n",
    "    # TODO: If prompt empty, ask for input\n",
    "    x = tokenizer(prompt).to(device)\n",
    "    y = torch.tensor([]).to(device)\n",
    "    for _ in range(steps):\n",
    "        logits, _ = model(x)\n",
    "        _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "        y = torch.cat((y, idx_next), dim=1)\n",
    "    return tokenizer.decode(y.cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answerer(question):\n",
    "    prompt = \"Q: \" + question + \"\\nA:\"\n",
    "    result = generate(prompt=prompt, steps=20)\n",
    "    answer = result.split(\"\\n\")[1][3:].split(\".\")[0] + \".\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.Interface(\n",
    "    title=\"My ZenML Chatbot\", \n",
    "    fn=question_answerer,\n",
    "    inputs=[\"text\"], \n",
    "    outputs=[\"text\"]\n",
    ").launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train GPT-XL on remote stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def load_pretrained_gpt_xl() -> GPT:\n",
    "    return GPT.from_pretrained(\"gpt2-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from zenml.integrations.seldon.steps import seldon_model_deployer_step\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from zenml.integrations.mlflow.steps import mlflow_model_deployer_step\n",
    "\n",
    "\n",
    "training_pipeline(\n",
    "    load_training_data=load_url_dataset(),\n",
    "    model_definition=load_pretrained_gpt_xl(),\n",
    "    trainer=train_llm(),\n",
    "    deployment_trigger=deployment_trigger(),\n",
    "    model_deployer=mlflow_model_deployer_step(),\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quickstart helps you get your first practical experience with ZenML and gives you a brief overview of various MLOps terms. \n",
    "\n",
    "Throughout this quickstart, we will:\n",
    "- Train a model, evaluate it, register the model version, deploy it, and embed it in an inference pipeline,\n",
    "- Automatically version, track, and cache data, models, and other artifacts,\n",
    "- Track model hyperparameters and metrics in an experiment tracking tool,\n",
    "- Measure and visualize train-test skew, training-serving skew, and data drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the code, let us briefly introduce you to some of the \n",
    "fundamental concepts of ZenML that we will use in this quickstart. If you are \n",
    "already familiar with these concepts, feel free to skip to the next section.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "The first concept that we will cover is the ZenML **Step**. In \n",
    "ZenML, a step provides a simple python interface to our users to design a \n",
    "stand-alone process in an ML workflow. They consume input artifacts \n",
    "and generate output artifacts. As an example, we can take a closer look at a \n",
    "simple step example:\n",
    "\n",
    "```python\n",
    "from zenml.steps import step\n",
    "\n",
    "@step\n",
    "def my_dataset_loader() -> pd.DataFrame:\n",
    "    \"\"\"My dataset loader step.\"\"\"\n",
    "    # Implement logic here and return the dataset...\n",
    "    return ...\n",
    "```\n",
    "\n",
    "#### Pipelines\n",
    "\n",
    "Following the steps, you will go over the concepts of **Pipelines**. These \n",
    "pipelines provide our users a simple python interface to design their ML \n",
    "workflows by linking different steps together. For instance, a very \n",
    "simple pipeline might look like this:\n",
    "\n",
    "```python\n",
    "from zenml.pipelines import pipeline\n",
    "\n",
    "@pipeline\n",
    "def my_pipeline(\n",
    "    my_data_loader,\n",
    "    my_model_trainer,\n",
    "):\n",
    "    \"\"\"Load the dataset and train a model.\"\"\"\n",
    "    dataset = my_data_loader()\n",
    "    model = my_model_trainer(dataset=dataset)\n",
    "```\n",
    "\n",
    "#### Stacks & Stack Components\n",
    "\n",
    "As for the execution of these pipelines, you need a **stack**. In ZenML, \n",
    "a stack stands for a set of configurations of your MLOps tools and \n",
    "infrastructure. Each stack consists of multiple **stack components** and\n",
    "depending on their type, these components serve different purposes.\n",
    "\n",
    "If you look at some examples of different flavors of stack components, you \n",
    "will see examples such as:\n",
    "\n",
    "- [Airflow**Orchestrator**](https://docs.zenml.io/component-gallery/orchestrators/airflow) which orchestrates your ML workflows on Airflow \n",
    "- [MLflow**ExperimentTracker**](https://docs.zenml.io/component-gallery/experiment-trackers/mlflow) which can track your experiments with MLFlow\n",
    "- [Evidently**DataValidator**](https://docs.zenml.io/component-gallery/data-validators/evidently) which can help you validate your data\n",
    "\n",
    "Any such combination of tools and infrastructure can be registered as a \n",
    "separate stack in ZenML. Since ZenML code is tooling-independent, you can \n",
    "switch between stacks with a single command and then automatically execute your\n",
    "ML workflows on the desired stack without having to modify your code.\n",
    "\n",
    "#### Integrations\n",
    "\n",
    "Finally, ZenML comes equipped with a wide variety of stack components flavors. \n",
    "While some of these flavors come built-in with the ZenML package, the others \n",
    "are implemented as a part of one of our integrations. Since our quickstart \n",
    "features some of these integrations, you will see a practical example on how \n",
    "to use these integrations in the upcoming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNLEesHEyjkg"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the quickstart, we need to install some dependencies. Once you have ZenML installed, you can use our CLI to install the required integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"zenml[server]\"  # install ZenML\n",
    "!zenml integration install sklearn mlflow evidently -y  # install ZenML integrations\n",
    "!zenml init  # Initialize a ZenML repository\n",
    "%pip install pyparsing==2.4.2  # required for Colab\n",
    "\n",
    "import IPython\n",
    "\n",
    "# automatically restart kernel\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please wait for the installation to complete before running subsequent cells. At the end of the installation, the notebook kernel will automatically restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google Colab\n",
    "\n",
    "If you follow this quickstart in Google's Colab, you will need an [ngrok account](https://dashboard.ngrok.com/signup) to view some of the visualizations later. Please set up an account, then set your user token below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGROK_TOKEN = \"\"  # TODO: set your ngrok token if you are working on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "\n",
    "if Environment.in_google_colab():  # Colab only setup\n",
    "    # install ngrok and set auth token\n",
    "    !pip install pyngrok\n",
    "    !ngrok authtoken {NGROK_TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an MLOps Stack\n",
    "\n",
    "ZenML decouples your code from the infrastructure and tooling you use.\n",
    "This enables you to quickly take your code from experimentation to production.\n",
    "Furthermore, using ZenML prevents vendor lock-in by allowing you to switch out any part of your MLOps stack easily.\n",
    "See the [ZenML Integrations](https://zenml.io/integrations) page for a list of all tools we currently support.\n",
    "\n",
    "Throughout this quickstart, we will use the following MLOps stack: A local orchestrator, a local artifact store, [MLFlow](https://mlflow.org/) experiment tracker and model deployer, and an [Evidently](https://evidentlyai.com/) data validator.\n",
    "\n",
    "![Quickstart MLOps Stack Overview](_assets/stack_overview_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to register all stack components that require configuration into our ZenML MLOps stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the MLflow experiment tracker\n",
    "!zenml experiment-tracker register mlflow_tracker --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model registry\n",
    "!zenml model-registry register mlflow_registry --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model deployer\n",
    "!zenml model-deployer register mlflow_deployer --flavor=mlflow\n",
    "\n",
    "# Register the Evidently data validator\n",
    "!zenml data-validator register evidently_validator --flavor=evidently\n",
    "\n",
    "# Register a new stack with the new stack components\n",
    "!zenml stack register quickstart_stack -a default\\\n",
    "                                       -o default\\\n",
    "                                       -d mlflow_deployer\\\n",
    "                                       -e mlflow_tracker\\\n",
    "                                       -r mlflow_registry\\\n",
    "                                       -dv evidently_validator\\\n",
    "                                       --set\n",
    "\n",
    "# Visualize the current ZenML stack\n",
    "!zenml stack describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ML Pipelines\n",
    "Let us now use ZenML to write two ML pipelines for continuous training and serving.\n",
    "\n",
    "The training pipeline will:\n",
    "- Load the [iris flower classification dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html),\n",
    "- Train a model on the training data (and track hyperparameters using [MLFlow](https://mlflow.org/)),\n",
    "- Test the model on the test data,\n",
    "- Register the model (with [MLFlow](https://mlflow.org/))\n",
    "\n",
    "The inference pipeline will:\n",
    "- Load inference data,\n",
    "- Deploy a chosen version of registered model,\n",
    "- Run model inference on the inference data,\n",
    "- Check for data drift (with [Evidently](https://evidentlyai.com/)).\n",
    "\n",
    "You can see a visualization of the two pipelines below:\n",
    "\n",
    "![Overview of Quickstart Pipelines](_assets/quickstart_pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define those pipelines with ZenML. To do so, we simply write a Python function that defines how the data will move through the different steps and decorate it with ZenML's `@pipeline` decorator. Under the hood, ZenML will build a [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph) that determines the order in which the steps need to be executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def training_pipeline(\n",
    "    training_data_loader,\n",
    "    trainer,\n",
    "    evaluator,\n",
    "    model_register,\n",
    "):\n",
    "    \"\"\"Train, evaluate, and deploy a model.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = training_data_loader()\n",
    "    model = trainer(X_train=X_train, y_train=y_train)\n",
    "    test_acc = evaluator(X_test=X_test, y_test=y_test, model=model)\n",
    "    model_register(model)\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def inference_pipeline(\n",
    "    inference_data_loader,\n",
    "    mlflow_model_deployer,\n",
    "    predictor,\n",
    "    training_data_loader,\n",
    "    drift_detector,\n",
    "):\n",
    "    \"\"\"Inference pipeline with skew and drift detection.\"\"\"\n",
    "    inference_data = inference_data_loader()\n",
    "    model_deployment_service = mlflow_model_deployer()\n",
    "    predictor(model_deployment_service, inference_data)\n",
    "    training_data, _, _, _ = training_data_loader()\n",
    "    drift_detector(training_data, inference_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Pipeline Steps\n",
    "\n",
    "Next, we need to implement the steps that make up these pipelines. \n",
    "Again, we can do this by writing simple Python functions and decorating them with ZenML's `@step` decorator.\n",
    "\n",
    "In total, we will need ten steps:\n",
    "- Training data loader\n",
    "- Inference data loader\n",
    "- Model trainer\n",
    "- Model evaluator\n",
    "- Model registerer\n",
    "- Inference data loader\n",
    "- Registered model deployer\n",
    "- Predictor\n",
    "- Skew comparison\n",
    "- Drift detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "Let's start with data loading. We load the iris dataset for training and, for simplicity, use some random samples for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from zenml.steps import Output, step\n",
    "\n",
    "\n",
    "@step\n",
    "def training_data_loader() -> Output(\n",
    "    X_train=pd.DataFrame,\n",
    "    X_test=pd.DataFrame,\n",
    "    y_train=pd.Series,\n",
    "    y_test=pd.Series,\n",
    "):\n",
    "    \"\"\"Load the iris dataset as tuple of Pandas DataFrame / Series.\"\"\"\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        iris.data, iris.target, test_size=0.2, shuffle=True, random_state=42\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def inference_data_loader() -> pd.DataFrame:\n",
    "    \"\"\"Load some (random) inference data.\"\"\"\n",
    "    return pd.DataFrame(\n",
    "        data=np.random.rand(10, 4) * 10,  # assume range [0, 10]\n",
    "        columns=load_iris(as_frame=True).data.columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Trainer\n",
    "To train our model, we define two steps that use the [sklearn SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model and [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) classifier and fit them on the given training data. Additionally, we log all model hyperparameters and metrics to [MLFlow](https://mlflow.org/).\n",
    "\n",
    "Note that we do not need to save the model within the step explicitly; ZenML is automatically taking care of this for us. Under the hood, ZenML persists all step inputs and outputs in an [Artifact Store](https://docs.zenml.io/component-gallery/artifact-stores). This also means that all of our data and models are automatically versioned and tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from zenml.client import Client\n",
    "\n",
    "experiment_tracker = Client().active_stack.experiment_tracker\n",
    "\n",
    "@step(enable_cache=False, experiment_tracker=experiment_tracker.name)\n",
    "def svc_trainer_mlflow(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train a sklearn SVC classifier and log to MLflow.\"\"\"\n",
    "    mlflow.sklearn.autolog()  # log all model hparams and metrics to MLflow\n",
    "    model = SVC(gamma=0.01)\n",
    "    model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(f\"Train accuracy: {train_acc}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from zenml.client import Client\n",
    "from zenml.integrations.mlflow.steps import mlflow_model_deployer_step, MLFlowDeployerParameters\n",
    "\n",
    "experiment_tracker = Client().active_stack.experiment_tracker\n",
    "\n",
    "@step(enable_cache=False, experiment_tracker=experiment_tracker.name)\n",
    "def tree_trainer_mlflow(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train a decision tree classifier and log to MLflow.\"\"\"\n",
    "    mlflow.sklearn.autolog()  # log all model hparams and metrics to MLflow\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(f\"Train accuracy: {train_acc}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluator and Deployment Trigger\n",
    "\n",
    "Since our model is a [sklearn Model](https://scikit-learn.org/stable/developers/develop.html), we can simply call `model.score` to compute its test accuracy.\n",
    "\n",
    "We then use the output of this step to only trigger deployment for models that achieved >90% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def evaluator(\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    model: ClassifierMixin,\n",
    ") -> float:\n",
    "    \"\"\"Calculate the accuracy on the test set\"\"\"\n",
    "    test_acc = model.score(X_test.to_numpy(), y_test.to_numpy())\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def deployment_trigger(test_acc: float) -> bool:\n",
    "    \"\"\"Only deploy if the test accuracy > 90%.\"\"\"\n",
    "    return test_acc > 0.9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Registry, Deployer and Drift Detection\n",
    "\n",
    "ZenML provides default steps for MLflow model registry, deployment and Evidently drift detection, which we can simply import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.mlflow.steps.mlflow_deployer import MLFlowDeployerParameters, mlflow_model_registry_deployer_step\n",
    "from zenml.integrations.mlflow.steps.mlflow_registry import MLFlowRegistryParameters, mlflow_register_model_step\n",
    "from zenml.model_registries.base_model_registry import ModelRegistryModelMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.evidently.steps import (\n",
    "    EvidentlyProfileParameters,\n",
    "    evidently_profile_step,\n",
    ")\n",
    "\n",
    "evidently_profile_params = EvidentlyProfileParameters(\n",
    "    profile_sections=[\"datadrift\"]\n",
    ")\n",
    "drift_detector = evidently_profile_step(\n",
    "    step_name=\"drift_detector\", params=evidently_profile_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Service Loader and Predictor\n",
    "\n",
    "Lastly, we need to write the inference pipeline steps for loading a deployed model and computing its prediction on the test data.\n",
    "\n",
    "To load the deployed model, we query ZenML's artifact store to find a model deployed with our current MLOps stack and the given training pipeline and deployment step names (more on this later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.services import BaseService\n",
    "from zenml.client import Client\n",
    "\n",
    "\n",
    "@step(enable_cache=False)\n",
    "def prediction_service_loader() -> BaseService:\n",
    "    \"\"\"Load the model service of our train_evaluate_deploy_pipeline.\"\"\"\n",
    "    client = Client()\n",
    "    model_deployer = client.active_stack.model_deployer\n",
    "    services = model_deployer.find_model_server(\n",
    "        pipeline_name=\"training_pipeline\",\n",
    "        pipeline_step_name=\"model_deployer\",\n",
    "        running=True,\n",
    "    )\n",
    "    service = services[0]\n",
    "    return service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inference the deployed model, we simply call its `predict()` method to get logits and compute the `argmax` to obtain the final prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def predictor(\n",
    "    service: BaseService,\n",
    "    data: pd.DataFrame,\n",
    ") -> Output(predictions=list):\n",
    "    \"\"\"Run a inference request against a prediction service\"\"\"\n",
    "    service.start(timeout=10)  # should be a NOP if already started\n",
    "    prediction = service.predict(data.to_numpy())\n",
    "    prediction = prediction.argmax(axis=-1)\n",
    "    print(f\"Prediction is: {[prediction.tolist()]}\")\n",
    "    return [prediction.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline and continuously deploy with caching\n",
    "\n",
    "Running pipelines is as simple as calling the `run()` method on an instance of the defined pipeline. Let's connect the concrete step functions to our defined pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline(\n",
    "    training_data_loader=training_data_loader(),\n",
    "    trainer=svc_trainer_mlflow(),\n",
    "    evaluator=evaluator(),\n",
    "    model_register=mlflow_register_model_step(\n",
    "            params=MLFlowRegistryParameters(\n",
    "                name=\"zenml-quickstart-model\",\n",
    "                metadata=ModelRegistryModelMetadata(\n",
    "                    gamma=0.01, arch=\"svc\"\n",
    "                ),\n",
    "                description=f\"The first run of the Quickstart pipeline.\",\n",
    "            )\n",
    "        ),\n",
    ").run(unlisted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's replace the SVC trainer with the Tree trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline(\n",
    "    training_data_loader=training_data_loader(),\n",
    "    trainer=tree_trainer_mlflow(),\n",
    "    evaluator=evaluator(),\n",
    "    model_register=mlflow_register_model_step(\n",
    "            params=MLFlowRegistryParameters(\n",
    "                name=\"zenml-quickstart-model\",\n",
    "                metadata=ModelRegistryModelMetadata(\n",
    "                    arch=\"decision_tree\"\n",
    "                ),\n",
    "                description=f\"The second run of the Quickstart pipeline.\",\n",
    "            )\n",
    "        ),\n",
    ").run(unlisted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the second pipeline ran slightly faster than the first? That's because ZenML understands that the `data_loader` step of your pipeline is unchanged, so it just reloads the output from your previous run and goes straight to the trainer part. This saves valuable time as you iterate on your pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference pipeline to deploy and inference on the registered model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training pipeline runs have finished, the trained model will have been registered using MLflow Model registry. We can use `zenml model-registry models list` to get an overview of all currently registered models and `zenml model-registry models list-versions` to get an overview of all versions of a specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml model-registry models list\n",
    "\n",
    "!zenml model-registry models list-versions zenml-quickstart-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the inference pipeline, the `mlflow_model_registry_deployer_step` will load the given model version and deploy it locally. After that, the `predictor` step will use the deployed model service to make predictions on the inference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline(\n",
    "        inference_data_loader=inference_data_loader(),\n",
    "        mlflow_model_deployer=mlflow_model_registry_deployer_step(\n",
    "            params=MLFlowDeployerParameters(\n",
    "                registry_model_name=\"zenml-quickstart-model\",\n",
    "                registry_model_version=\"1\",\n",
    "                # or you can use the model stage if you have set it in the MLflow registry\n",
    "                # registered_model_stage=\"None\" # \"Staging\", \"Production\", \"Archived\"\n",
    "            )\n",
    "        ),\n",
    "        predictor=predictor(),\n",
    "        training_data_loader=training_data_loader(),\n",
    "        drift_detector=drift_detector,\n",
    "    ).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run `zenml model-deployer models list` to get an overview of all currently deployed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml model-deployer models list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZenML dashboard\n",
    "\n",
    "Once the pipeline runs have completed, we can visualize all of our ZenML \n",
    "resources in the ZenML dashboard. \n",
    "In order to spin up the dashboard, please execute the following code cell.\n",
    "\n",
    "**Colab Note:** On Colab, you can access the ZenML dashboard via the \n",
    "`...ngrok.io` URL that will be shown in the first line of the output of the \n",
    "following code cell.\n",
    "Please wait for the server to fully start up before accessing the dashboard URL, \n",
    "otherwise some resources might not have been fully loaded yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "from zenml.integrations.mlflow.mlflow_utils import get_tracking_uri\n",
    "\n",
    "\n",
    "def start_zenml_dashboard(port=8237):\n",
    "    if Environment.in_google_colab():\n",
    "        from pyngrok import ngrok\n",
    "\n",
    "        public_url = ngrok.connect(port)\n",
    "        print(f\"\\x1b[31mIn Colab, use this URL instead: {public_url}!\\x1b[0m\")\n",
    "        !zenml up --blocking --port {port}\n",
    "\n",
    "    else:\n",
    "        !zenml up --port {port}\n",
    "\n",
    "start_zenml_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a local ZenML server and connect you to it. Once connected, \n",
    "the dashboard will be available for you at the URL displayed in the command\n",
    "output above. You can login with username `default` and an empty password.\n",
    "\n",
    "![ZenML Server Up](_assets/zenml-up.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this dashboard, you will be able to manage your pipelines and the corresponding pipeline runs, your stacks and stack components and your personal settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data Skew and Data Drift\n",
    "\n",
    "ZenML provides a variety of visualization tools in addition dashboard shown above. E.g., using the `EvidentlyVisualizer` we can visualize data drift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.evidently.visualizers import EvidentlyVisualizer\n",
    "\n",
    "inference_run = inference_pipeline.get_runs()[0]\n",
    "drift_detection_step = inference_run.get_step(step=\"drift_detector\")\n",
    "\n",
    "EvidentlyVisualizer().visualize(drift_detection_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, Evidently will also detect data drift for all four features:\n",
    "\n",
    "<img src=\"_assets/data_drift.png\" alt=\"Evidently Data Drift Visualization\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Experiment Tracking\n",
    "\n",
    "Lastly, remember how we added MLflow experiment tracking to our `svc_trainer_mlflow` step before?\n",
    "Those two simple lines of code automatically configured and initialized MLflow and logged all hyperparameters and metrics there.\n",
    "\n",
    "Let's start up the MLflow UI and check it out!\n",
    "\n",
    "**Colab Note:** On Colab, you can access the MLflow UI via the `...ngrok.io` URL\n",
    "that will be shown in the first line of the output of the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "from zenml.integrations.mlflow.mlflow_utils import get_tracking_uri\n",
    "\n",
    "\n",
    "def open_mlflow_ui(port=4997):\n",
    "    if Environment.in_google_colab():\n",
    "        from pyngrok import ngrok\n",
    "\n",
    "        public_url = ngrok.connect(port)\n",
    "        print(f\"\\x1b[31mIn Colab, use this URL instead: {public_url}!\\x1b[0m\")\n",
    "\n",
    "    !mlflow ui --backend-store-uri=\"{get_tracking_uri()}\" --port={port}\n",
    "\n",
    "\n",
    "open_mlflow_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLflow UI](_assets/mlflow_ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You just built your first ML Pipeline! You not only trained a model, you also deployed it, served it, and learned how to monitor and visualize everything that's going on. Did you notice how easy it was to bring all of the different components together using ZenML's abstractions? And that is just the tip of the iceberg of what ZenML can do; check out the [**Integrations**](https://zenml.io/integrations) page for a list of all the cool MLOps tools that ZenML supports!\n",
    "\n",
    "To improve upon the ML workflows we built in this quickstart, you could, for instance:\n",
    "- [Deploy ZenML on the Cloud]() to collaborate with your teammates,\n",
    "- Experiment with more sophisticated models, such as [XGBoost](https://zenml.io/integrations/xgboost),\n",
    "- Set up automated [Slack alerts](https://zenml.io/integrations/zen-ml-slack-integration) to get notified when data drift happens,\n",
    "- Run the pipelines on scalable, distributed stacks like [Kubeflow](https://zenml.io/integrations/kubeflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go next\n",
    "\n",
    "* If you have questions or feedback... \n",
    "  * Join our [**Slack Community**](https://zenml.io/slack-invite) and become part of the ZenML family!\n",
    "* If this quickstart was a bit too quick for you... \n",
    "  * Check out [**ZenBytes**](https://github.com/zenml-io/zenbytes), our lesson series on practical MLOps, where we cover each MLOps concept in much more detail.\n",
    "* If you want to learn more about using or extending ZenML...\n",
    "  * Check out our [**Docs**](https://docs.zenml.io/) or read through our code on [**Github**](https://github.com/zenml-io/zenml).\n",
    "* If you want to quickly learn how to use a specific tool with ZenML...\n",
    "  * Check out our collection of [**Examples**](https://github.com/zenml-io/zenml/tree/doc/hamza-misc-updates/examples).\n",
    "* If you want to see some advanced ZenML use cases... \n",
    "  * Check out [**ZenML Projects**](https://github.com/zenml-io/zenml-projects), our collection of production-grade ML use-cases."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ZenML Quickstart.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d2a2855dd99d151bf9fdc91431d39c1e05805b2488b8cd3e8da8d54747db678"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
