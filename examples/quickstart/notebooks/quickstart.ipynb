{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZenML Quickstart Guide\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/notebooks/quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "ZenML is an extensible, open-source MLOps framework for creating portable, \n",
    "production-ready MLOps pipelines.\n",
    "\n",
    "ZenML pipelines are infrastructure-agnostic so you can switch between \n",
    "development and production environments without requiring any code changes. \n",
    "This gives both Data Scientists and MLOps Engineers more freedom and \n",
    "independence in how they approach their work:\n",
    "\n",
    "TODO: Architecture / Workflow visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZenML for MLOps Engineers\n",
    "\n",
    "As an MLOps expert, ZenML enables you to define, deploy, and manage\n",
    "sophisticated production infrastructure and tooling setups that are easy to \n",
    "share with your colleagues.\n",
    "\n",
    "Since infrastructure is decoupled from code, you are never vendor-locked. With\n",
    "ZenML you have the freedom to switch to a different tooling stack whenever it \n",
    "suites you:\n",
    "\n",
    "```bash\n",
    "zenml stack set gcp\n",
    "python run.py  # Run your ML workflows in GCP\n",
    "zenml stack set aws\n",
    "python run.py  # Now your ML workflow runs in AWS\n",
    "```\n",
    "\n",
    "To share your tooling stack with your colleagues, simply set up a ZenML server, \n",
    "register your production environment as a ZenML stack, and invite your \n",
    "colleagues to run their ML workflows on it:\n",
    "\n",
    "```bash\n",
    "zenml deploy  # Deploy ZenML\n",
    "zenml stack register production ...  # Register your production environment\n",
    "zenml stack share production  # Make it available to your colleagues\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RO_v5iIaYFi2"
   },
   "source": [
    "### ZenML for Data Scientists\n",
    "\n",
    "As a Data Scientists, you can develop ML models locally with all your \n",
    "favourite tools without having to worry about production issues at all. Once you\n",
    "are happy with your results, you can switch to a production environment via a\n",
    "single command and all the code you wrote will just work:\n",
    "\n",
    "```bash\n",
    "python run.py  # develop your code locally with all your favourite tools\n",
    "zenml stack set production\n",
    "python run.py  # run your code in production without any code changes\n",
    "```\n",
    "\n",
    "ZenML is designed to be as unintrusive as possible. Adding a ZenML `@step` or\n",
    "`@pipeline` decorator to your Python functions is enough to turn your current \n",
    "code into ZenML pipelines:\n",
    "\n",
    "```python\n",
    "@step\n",
    "def step_1() -> str:\n",
    "  return \"world\"\n",
    "\n",
    "@step\n",
    "def step_2(input_one: str, input_two: str) -> None:\n",
    "  combined_str = input_one + ' ' + input_two\n",
    "  print(combined_str)\n",
    "\n",
    "@pipeline\n",
    "def my_pipeline():\n",
    "  output_step_one = step_1()\n",
    "  step_2(input_one=\"hello\", input_two=output_step_one)\n",
    "\n",
    "my_pipeline()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see it in action and use ZenML to bring an LLM training and deployment\n",
    "workflow into production. As an example, we will first train and deploy a\n",
    "simple LLM model locally. Then we will switch the entire workflow to a\n",
    "production environment in the cloud that will automatically train the\n",
    "model on GPU-enabled hardware and deploy it to a scalable Kubernetes cluster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install pytorch mlflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml hub install mingpt_example mlflow_steps -y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Local Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the MLflow experiment tracker\n",
    "!zenml experiment-tracker register mlflow_tracker --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model registry\n",
    "!zenml model-registry register mlflow_registry --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model deployer\n",
    "!zenml model-deployer register mlflow_deployer --flavor=mlflow\n",
    "\n",
    "# Register a new stack with the new stack components\n",
    "!zenml stack register quickstart_stack -a default\\\n",
    "                                       -o default\\\n",
    "                                       -d mlflow_deployer\\\n",
    "                                       -e mlflow_tracker\\\n",
    "                                       -r mlflow_registry\\\n",
    "\n",
    "!zenml stack set quickstart_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and deploy GPT-nano locally\n",
    "TODO: show source code of steps here and motivate the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.hub.mingpt_example import url_dataset_loader_step, gpt_nano_loader_step, pretrained_gpt_xl_loader_step, mingpt_trainer_step\n",
    "from zenml.hub.mlflow_steps.mlflow_deployer import mlflow_model_deployer_step\n",
    "# from zenml.integrations.mlflow.steps import mlflow_model_deployer_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.client import Client\n",
    "\n",
    "experiment_tracker = Client().active_stack.experiment_tracker\n",
    "\n",
    "mingpt_trainer = mingpt_trainer_step()\n",
    "\n",
    "mingpt_trainer.configure(experiment_tracker=experiment_tracker.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Delete step below once new pipeline definition is life and simply set decision=True in deployer step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.steps import step\n",
    "\n",
    "@step\n",
    "def deployment_trigger() -> bool:\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Switch to new pipeline definition style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "\n",
    "@pipeline(enable_cache=False)\n",
    "def training_pipeline(\n",
    "    load_training_data,\n",
    "    model_definition,\n",
    "    trainer,\n",
    "    deployment_trigger,\n",
    "    model_deployer,\n",
    "):\n",
    "    \"\"\"Train, evaluate, and deploy a model.\"\"\"\n",
    "    dataset = load_training_data()\n",
    "    model = model_definition()\n",
    "    model = trainer(dataset, model)\n",
    "    deployment_decision = deployment_trigger()\n",
    "    model_deployer(deployment_decision, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = training_pipeline(\n",
    "    load_training_data=url_dataset_loader_step(),\n",
    "    model_definition=gpt_nano_loader_step(),\n",
    "    trainer=mingpt_trainer,\n",
    "    deployment_trigger=deployment_trigger(),\n",
    "    model_deployer=mlflow_model_deployer_step(),\n",
    ")\n",
    "pip.run(enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml model-deployer models list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Show run in dashboard TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spin up gradio app\n",
    "\n",
    "### TODO: use deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = pip.get_runs()[0]\n",
    "deployer_step = pipeline_run.get_step(\"model_deployer\")\n",
    "deployed_model_url = deployer_step.metadata[\"deployed_model_url\"].value\n",
    "deployed_model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_url = \"http://127.0.0.1:8002/invocations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "def send_request_to_deployed_model(data):\n",
    "    response = requests.post(\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        url=deployed_model_url,\n",
    "        data=json.dumps({\"instances\": data.numpy().tolist()})\n",
    "    ).json()\n",
    "    if \"predictions\" in response:\n",
    "        return torch.tensor(response[\"predictions\"])\n",
    "    else:\n",
    "        raise Exception(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from zenml.hub.mingpt_example.mingpt.bpe import BPETokenizer\n",
    "\n",
    "def generate(prompt, steps):\n",
    "    tokenizer = BPETokenizer()\n",
    "    # TODO: If prompt empty, ask for input\n",
    "    tokens = tokenizer(prompt)\n",
    "    for _ in range(steps):\n",
    "        logits = send_request_to_deployed_model(tokens)\n",
    "        _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "        tokens = torch.cat((tokens, idx_next.reshape(tokens.shape)), dim=1)\n",
    "    return tokenizer.decode(tokens.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answerer(question):\n",
    "    prompt = \"Question: \" + question + \"\\nAnswer: \"\n",
    "    result = generate(prompt=prompt, steps=2)\n",
    "    answer = result#.split(\"\\n\")[1][8:].split(\".\")[0] + \".\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.Interface(\n",
    "    title=\"My ZenML Chatbot\", \n",
    "    fn=question_answerer,\n",
    "    inputs=[\"text\"], \n",
    "    outputs=[\"text\"]\n",
    ").launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train GPT-XL on remote stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from zenml.integrations.seldon.steps import seldon_model_deployer_step\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from zenml.integrations.mlflow.steps import mlflow_model_deployer_step\n",
    "from zenml.hub.quickstart_example pretrained_gpt_xl_loader_step\n",
    "\n",
    "training_pipeline(\n",
    "    load_training_data=load_url_dataset(),\n",
    "    model_definition=load_pretrained_gpt_xl(),\n",
    "    trainer=train_llm(),\n",
    "    deployment_trigger=deployment_trigger(),\n",
    "    model_deployer=mlflow_model_deployer_step(),\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quickstart helps you get your first practical experience with ZenML and gives you a brief overview of various MLOps terms. \n",
    "\n",
    "Throughout this quickstart, we will:\n",
    "- Train a model, evaluate it, register the model version, deploy it, and embed it in an inference pipeline,\n",
    "- Automatically version, track, and cache data, models, and other artifacts,\n",
    "- Track model hyperparameters and metrics in an experiment tracking tool,\n",
    "- Measure and visualize train-test skew, training-serving skew, and data drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the code, let us briefly introduce you to some of the \n",
    "fundamental concepts of ZenML that we will use in this quickstart. If you are \n",
    "already familiar with these concepts, feel free to skip to the next section.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "The first concept that we will cover is the ZenML **Step**. In \n",
    "ZenML, a step provides a simple python interface to our users to design a \n",
    "stand-alone process in an ML workflow. They consume input artifacts \n",
    "and generate output artifacts. As an example, we can take a closer look at a \n",
    "simple step example:\n",
    "\n",
    "```python\n",
    "from zenml.steps import step\n",
    "\n",
    "@step\n",
    "def my_dataset_loader() -> pd.DataFrame:\n",
    "    \"\"\"My dataset loader step.\"\"\"\n",
    "    # Implement logic here and return the dataset...\n",
    "    return ...\n",
    "```\n",
    "\n",
    "#### Pipelines\n",
    "\n",
    "Following the steps, you will go over the concepts of **Pipelines**. These \n",
    "pipelines provide our users a simple python interface to design their ML \n",
    "workflows by linking different steps together. For instance, a very \n",
    "simple pipeline might look like this:\n",
    "\n",
    "```python\n",
    "from zenml.pipelines import pipeline\n",
    "\n",
    "@pipeline\n",
    "def my_pipeline(\n",
    "    my_data_loader,\n",
    "    my_model_trainer,\n",
    "):\n",
    "    \"\"\"Load the dataset and train a model.\"\"\"\n",
    "    dataset = my_data_loader()\n",
    "    model = my_model_trainer(dataset=dataset)\n",
    "```\n",
    "\n",
    "#### Stacks & Stack Components\n",
    "\n",
    "As for the execution of these pipelines, you need a **stack**. In ZenML, \n",
    "a stack stands for a set of configurations of your MLOps tools and \n",
    "infrastructure. Each stack consists of multiple **stack components** and\n",
    "depending on their type, these components serve different purposes.\n",
    "\n",
    "If you look at some examples of different flavors of stack components, you \n",
    "will see examples such as:\n",
    "\n",
    "- [Airflow**Orchestrator**](https://docs.zenml.io/component-gallery/orchestrators/airflow) which orchestrates your ML workflows on Airflow \n",
    "- [MLflow**ExperimentTracker**](https://docs.zenml.io/component-gallery/experiment-trackers/mlflow) which can track your experiments with MLFlow\n",
    "- [Evidently**DataValidator**](https://docs.zenml.io/component-gallery/data-validators/evidently) which can help you validate your data\n",
    "\n",
    "Any such combination of tools and infrastructure can be registered as a \n",
    "separate stack in ZenML. Since ZenML code is tooling-independent, you can \n",
    "switch between stacks with a single command and then automatically execute your\n",
    "ML workflows on the desired stack without having to modify your code.\n",
    "\n",
    "#### Integrations\n",
    "\n",
    "Finally, ZenML comes equipped with a wide variety of stack components flavors. \n",
    "While some of these flavors come built-in with the ZenML package, the others \n",
    "are implemented as a part of one of our integrations. Since our quickstart \n",
    "features some of these integrations, you will see a practical example on how \n",
    "to use these integrations in the upcoming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNLEesHEyjkg"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the quickstart, we need to install some dependencies. Once you have ZenML installed, you can use our CLI to install the required integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"zenml[server]\"  # install ZenML\n",
    "!zenml integration install sklearn mlflow evidently -y  # install ZenML integrations\n",
    "!zenml init  # Initialize a ZenML repository\n",
    "%pip install pyparsing==2.4.2  # required for Colab\n",
    "\n",
    "import IPython\n",
    "\n",
    "# automatically restart kernel\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please wait for the installation to complete before running subsequent cells. At the end of the installation, the notebook kernel will automatically restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google Colab\n",
    "\n",
    "If you follow this quickstart in Google's Colab, you will need an [ngrok account](https://dashboard.ngrok.com/signup) to view some of the visualizations later. Please set up an account, then set your user token below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGROK_TOKEN = \"\"  # TODO: set your ngrok token if you are working on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "\n",
    "if Environment.in_google_colab():  # Colab only setup\n",
    "    # install ngrok and set auth token\n",
    "    !pip install pyngrok\n",
    "    !ngrok authtoken {NGROK_TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an MLOps Stack\n",
    "\n",
    "ZenML decouples your code from the infrastructure and tooling you use.\n",
    "This enables you to quickly take your code from experimentation to production.\n",
    "Furthermore, using ZenML prevents vendor lock-in by allowing you to switch out any part of your MLOps stack easily.\n",
    "See the [ZenML Integrations](https://zenml.io/integrations) page for a list of all tools we currently support.\n",
    "\n",
    "Throughout this quickstart, we will use the following MLOps stack: A local orchestrator, a local artifact store, [MLFlow](https://mlflow.org/) experiment tracker and model deployer, and an [Evidently](https://evidentlyai.com/) data validator.\n",
    "\n",
    "![Quickstart MLOps Stack Overview](_assets/stack_overview_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to register all stack components that require configuration into our ZenML MLOps stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the MLflow experiment tracker\n",
    "!zenml experiment-tracker register mlflow_tracker --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model registry\n",
    "!zenml model-registry register mlflow_registry --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model deployer\n",
    "!zenml model-deployer register mlflow_deployer --flavor=mlflow\n",
    "\n",
    "# Register the Evidently data validator\n",
    "!zenml data-validator register evidently_validator --flavor=evidently\n",
    "\n",
    "# Register a new stack with the new stack components\n",
    "!zenml stack register quickstart_stack -a default\\\n",
    "                                       -o default\\\n",
    "                                       -d mlflow_deployer\\\n",
    "                                       -e mlflow_tracker\\\n",
    "                                       -r mlflow_registry\\\n",
    "                                       -dv evidently_validator\\\n",
    "                                       --set\n",
    "\n",
    "# Visualize the current ZenML stack\n",
    "!zenml stack describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ML Pipelines\n",
    "Let us now use ZenML to write two ML pipelines for continuous training and serving.\n",
    "\n",
    "The training pipeline will:\n",
    "- Load the [iris flower classification dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html),\n",
    "- Train a model on the training data (and track hyperparameters using [MLFlow](https://mlflow.org/)),\n",
    "- Test the model on the test data,\n",
    "- Register the model (with [MLFlow](https://mlflow.org/))\n",
    "\n",
    "The inference pipeline will:\n",
    "- Load inference data,\n",
    "- Deploy a chosen version of registered model,\n",
    "- Run model inference on the inference data,\n",
    "- Check for data drift (with [Evidently](https://evidentlyai.com/)).\n",
    "\n",
    "You can see a visualization of the two pipelines below:\n",
    "\n",
    "![Overview of Quickstart Pipelines](_assets/quickstart_pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define those pipelines with ZenML. To do so, we simply write a Python function that defines how the data will move through the different steps and decorate it with ZenML's `@pipeline` decorator. Under the hood, ZenML will build a [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph) that determines the order in which the steps need to be executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def training_pipeline(\n",
    "    training_data_loader,\n",
    "    trainer,\n",
    "    evaluator,\n",
    "    model_register,\n",
    "):\n",
    "    \"\"\"Train, evaluate, and deploy a model.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = training_data_loader()\n",
    "    model = trainer(X_train=X_train, y_train=y_train)\n",
    "    test_acc = evaluator(X_test=X_test, y_test=y_test, model=model)\n",
    "    model_register(model)\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def inference_pipeline(\n",
    "    inference_data_loader,\n",
    "    mlflow_model_deployer,\n",
    "    predictor,\n",
    "    training_data_loader,\n",
    "    drift_detector,\n",
    "):\n",
    "    \"\"\"Inference pipeline with skew and drift detection.\"\"\"\n",
    "    inference_data = inference_data_loader()\n",
    "    model_deployment_service = mlflow_model_deployer()\n",
    "    predictor(model_deployment_service, inference_data)\n",
    "    training_data, _, _, _ = training_data_loader()\n",
    "    drift_detector(training_data, inference_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Pipeline Steps\n",
    "\n",
    "Next, we need to implement the steps that make up these pipelines. \n",
    "Again, we can do this by writing simple Python functions and decorating them with ZenML's `@step` decorator.\n",
    "\n",
    "In total, we will need ten steps:\n",
    "- Training data loader\n",
    "- Inference data loader\n",
    "- Model trainer\n",
    "- Model evaluator\n",
    "- Model registerer\n",
    "- Inference data loader\n",
    "- Registered model deployer\n",
    "- Predictor\n",
    "- Skew comparison\n",
    "- Drift detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "Let's start with data loading. We load the iris dataset for training and, for simplicity, use some random samples for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from zenml.steps import Output, step\n",
    "\n",
    "\n",
    "@step\n",
    "def training_data_loader() -> Output(\n",
    "    X_train=pd.DataFrame,\n",
    "    X_test=pd.DataFrame,\n",
    "    y_train=pd.Series,\n",
    "    y_test=pd.Series,\n",
    "):\n",
    "    \"\"\"Load the iris dataset as tuple of Pandas DataFrame / Series.\"\"\"\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        iris.data, iris.target, test_size=0.2, shuffle=True, random_state=42\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def inference_data_loader() -> pd.DataFrame:\n",
    "    \"\"\"Load some (random) inference data.\"\"\"\n",
    "    return pd.DataFrame(\n",
    "        data=np.random.rand(10, 4) * 10,  # assume range [0, 10]\n",
    "        columns=load_iris(as_frame=True).data.columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Trainer\n",
    "To train our model, we define two steps that use the [sklearn SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model and [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) classifier and fit them on the given training data. Additionally, we log all model hyperparameters and metrics to [MLFlow](https://mlflow.org/).\n",
    "\n",
    "Note that we do not need to save the model within the step explicitly; ZenML is automatically taking care of this for us. Under the hood, ZenML persists all step inputs and outputs in an [Artifact Store](https://docs.zenml.io/component-gallery/artifact-stores). This also means that all of our data and models are automatically versioned and tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from zenml.client import Client\n",
    "\n",
    "experiment_tracker = Client().active_stack.experiment_tracker\n",
    "\n",
    "@step(enable_cache=False, experiment_tracker=experiment_tracker.name)\n",
    "def svc_trainer_mlflow(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train a sklearn SVC classifier and log to MLflow.\"\"\"\n",
    "    mlflow.sklearn.autolog()  # log all model hparams and metrics to MLflow\n",
    "    model = SVC(gamma=0.01)\n",
    "    model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(f\"Train accuracy: {train_acc}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from zenml.client import Client\n",
    "from zenml.integrations.mlflow.steps import mlflow_model_deployer_step, MLFlowDeployerParameters\n",
    "\n",
    "experiment_tracker = Client().active_stack.experiment_tracker\n",
    "\n",
    "@step(enable_cache=False, experiment_tracker=experiment_tracker.name)\n",
    "def tree_trainer_mlflow(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train a decision tree classifier and log to MLflow.\"\"\"\n",
    "    mlflow.sklearn.autolog()  # log all model hparams and metrics to MLflow\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(f\"Train accuracy: {train_acc}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluator and Deployment Trigger\n",
    "\n",
    "Since our model is a [sklearn Model](https://scikit-learn.org/stable/developers/develop.html), we can simply call `model.score` to compute its test accuracy.\n",
    "\n",
    "We then use the output of this step to only trigger deployment for models that achieved >90% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def evaluator(\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    model: ClassifierMixin,\n",
    ") -> float:\n",
    "    \"\"\"Calculate the accuracy on the test set\"\"\"\n",
    "    test_acc = model.score(X_test.to_numpy(), y_test.to_numpy())\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def deployment_trigger(test_acc: float) -> bool:\n",
    "    \"\"\"Only deploy if the test accuracy > 90%.\"\"\"\n",
    "    return test_acc > 0.9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Registry, Deployer and Drift Detection\n",
    "\n",
    "ZenML provides default steps for MLflow model registry, deployment and Evidently drift detection, which we can simply import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.mlflow.steps.mlflow_deployer import MLFlowDeployerParameters, mlflow_model_registry_deployer_step\n",
    "from zenml.integrations.mlflow.steps.mlflow_registry import MLFlowRegistryParameters, mlflow_register_model_step\n",
    "from zenml.model_registries.base_model_registry import ModelRegistryModelMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.evidently.steps import (\n",
    "    EvidentlyProfileParameters,\n",
    "    evidently_profile_step,\n",
    ")\n",
    "\n",
    "evidently_profile_params = EvidentlyProfileParameters(\n",
    "    profile_sections=[\"datadrift\"]\n",
    ")\n",
    "drift_detector = evidently_profile_step(\n",
    "    step_name=\"drift_detector\", params=evidently_profile_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Service Loader and Predictor\n",
    "\n",
    "Lastly, we need to write the inference pipeline steps for loading a deployed model and computing its prediction on the test data.\n",
    "\n",
    "To load the deployed model, we query ZenML's artifact store to find a model deployed with our current MLOps stack and the given training pipeline and deployment step names (more on this later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.services import BaseService\n",
    "from zenml.client import Client\n",
    "\n",
    "\n",
    "@step(enable_cache=False)\n",
    "def prediction_service_loader() -> BaseService:\n",
    "    \"\"\"Load the model service of our train_evaluate_deploy_pipeline.\"\"\"\n",
    "    client = Client()\n",
    "    model_deployer = client.active_stack.model_deployer\n",
    "    services = model_deployer.find_model_server(\n",
    "        pipeline_name=\"training_pipeline\",\n",
    "        pipeline_step_name=\"model_deployer\",\n",
    "        running=True,\n",
    "    )\n",
    "    service = services[0]\n",
    "    return service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inference the deployed model, we simply call its `predict()` method to get logits and compute the `argmax` to obtain the final prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def predictor(\n",
    "    service: BaseService,\n",
    "    data: pd.DataFrame,\n",
    ") -> Output(predictions=list):\n",
    "    \"\"\"Run a inference request against a prediction service\"\"\"\n",
    "    service.start(timeout=10)  # should be a NOP if already started\n",
    "    prediction = service.predict(data.to_numpy())\n",
    "    prediction = prediction.argmax(axis=-1)\n",
    "    print(f\"Prediction is: {[prediction.tolist()]}\")\n",
    "    return [prediction.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline and continuously deploy with caching\n",
    "\n",
    "Running pipelines is as simple as calling the `run()` method on an instance of the defined pipeline. Let's connect the concrete step functions to our defined pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline(\n",
    "    training_data_loader=training_data_loader(),\n",
    "    trainer=svc_trainer_mlflow(),\n",
    "    evaluator=evaluator(),\n",
    "    model_register=mlflow_register_model_step(\n",
    "            params=MLFlowRegistryParameters(\n",
    "                name=\"zenml-quickstart-model\",\n",
    "                metadata=ModelRegistryModelMetadata(\n",
    "                    gamma=0.01, arch=\"svc\"\n",
    "                ),\n",
    "                description=f\"The first run of the Quickstart pipeline.\",\n",
    "            )\n",
    "        ),\n",
    ").run(unlisted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's replace the SVC trainer with the Tree trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline(\n",
    "    training_data_loader=training_data_loader(),\n",
    "    trainer=tree_trainer_mlflow(),\n",
    "    evaluator=evaluator(),\n",
    "    model_register=mlflow_register_model_step(\n",
    "            params=MLFlowRegistryParameters(\n",
    "                name=\"zenml-quickstart-model\",\n",
    "                metadata=ModelRegistryModelMetadata(\n",
    "                    arch=\"decision_tree\"\n",
    "                ),\n",
    "                description=f\"The second run of the Quickstart pipeline.\",\n",
    "            )\n",
    "        ),\n",
    ").run(unlisted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the second pipeline ran slightly faster than the first? That's because ZenML understands that the `data_loader` step of your pipeline is unchanged, so it just reloads the output from your previous run and goes straight to the trainer part. This saves valuable time as you iterate on your pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference pipeline to deploy and inference on the registered model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training pipeline runs have finished, the trained model will have been registered using MLflow Model registry. We can use `zenml model-registry models list` to get an overview of all currently registered models and `zenml model-registry models list-versions` to get an overview of all versions of a specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml model-registry models list\n",
    "\n",
    "!zenml model-registry models list-versions zenml-quickstart-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the inference pipeline, the `mlflow_model_registry_deployer_step` will load the given model version and deploy it locally. After that, the `predictor` step will use the deployed model service to make predictions on the inference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline(\n",
    "        inference_data_loader=inference_data_loader(),\n",
    "        mlflow_model_deployer=mlflow_model_registry_deployer_step(\n",
    "            params=MLFlowDeployerParameters(\n",
    "                registry_model_name=\"zenml-quickstart-model\",\n",
    "                registry_model_version=\"1\",\n",
    "                # or you can use the model stage if you have set it in the MLflow registry\n",
    "                # registered_model_stage=\"None\" # \"Staging\", \"Production\", \"Archived\"\n",
    "            )\n",
    "        ),\n",
    "        predictor=predictor(),\n",
    "        training_data_loader=training_data_loader(),\n",
    "        drift_detector=drift_detector,\n",
    "    ).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run `zenml model-deployer models list` to get an overview of all currently deployed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml model-deployer models list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZenML dashboard\n",
    "\n",
    "Once the pipeline runs have completed, we can visualize all of our ZenML \n",
    "resources in the ZenML dashboard. \n",
    "In order to spin up the dashboard, please execute the following code cell.\n",
    "\n",
    "**Colab Note:** On Colab, you can access the ZenML dashboard via the \n",
    "`...ngrok.io` URL that will be shown in the first line of the output of the \n",
    "following code cell.\n",
    "Please wait for the server to fully start up before accessing the dashboard URL, \n",
    "otherwise some resources might not have been fully loaded yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "from zenml.integrations.mlflow.mlflow_utils import get_tracking_uri\n",
    "\n",
    "\n",
    "def start_zenml_dashboard(port=8237):\n",
    "    if Environment.in_google_colab():\n",
    "        from pyngrok import ngrok\n",
    "\n",
    "        public_url = ngrok.connect(port)\n",
    "        print(f\"\\x1b[31mIn Colab, use this URL instead: {public_url}!\\x1b[0m\")\n",
    "        !zenml up --blocking --port {port}\n",
    "\n",
    "    else:\n",
    "        !zenml up --port {port}\n",
    "\n",
    "start_zenml_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a local ZenML server and connect you to it. Once connected, \n",
    "the dashboard will be available for you at the URL displayed in the command\n",
    "output above. You can login with username `default` and an empty password.\n",
    "\n",
    "![ZenML Server Up](_assets/zenml-up.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this dashboard, you will be able to manage your pipelines and the corresponding pipeline runs, your stacks and stack components and your personal settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data Skew and Data Drift\n",
    "\n",
    "ZenML provides a variety of visualization tools in addition dashboard shown above. E.g., using the `EvidentlyVisualizer` we can visualize data drift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.evidently.visualizers import EvidentlyVisualizer\n",
    "\n",
    "inference_run = inference_pipeline.get_runs()[0]\n",
    "drift_detection_step = inference_run.get_step(step=\"drift_detector\")\n",
    "\n",
    "EvidentlyVisualizer().visualize(drift_detection_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, Evidently will also detect data drift for all four features:\n",
    "\n",
    "<img src=\"_assets/data_drift.png\" alt=\"Evidently Data Drift Visualization\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Experiment Tracking\n",
    "\n",
    "Lastly, remember how we added MLflow experiment tracking to our `svc_trainer_mlflow` step before?\n",
    "Those two simple lines of code automatically configured and initialized MLflow and logged all hyperparameters and metrics there.\n",
    "\n",
    "Let's start up the MLflow UI and check it out!\n",
    "\n",
    "**Colab Note:** On Colab, you can access the MLflow UI via the `...ngrok.io` URL\n",
    "that will be shown in the first line of the output of the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "from zenml.integrations.mlflow.mlflow_utils import get_tracking_uri\n",
    "\n",
    "\n",
    "def open_mlflow_ui(port=4997):\n",
    "    if Environment.in_google_colab():\n",
    "        from pyngrok import ngrok\n",
    "\n",
    "        public_url = ngrok.connect(port)\n",
    "        print(f\"\\x1b[31mIn Colab, use this URL instead: {public_url}!\\x1b[0m\")\n",
    "\n",
    "    !mlflow ui --backend-store-uri=\"{get_tracking_uri()}\" --port={port}\n",
    "\n",
    "\n",
    "open_mlflow_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLflow UI](_assets/mlflow_ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You just built your first ML Pipeline! You not only trained a model, you also deployed it, served it, and learned how to monitor and visualize everything that's going on. Did you notice how easy it was to bring all of the different components together using ZenML's abstractions? And that is just the tip of the iceberg of what ZenML can do; check out the [**Integrations**](https://zenml.io/integrations) page for a list of all the cool MLOps tools that ZenML supports!\n",
    "\n",
    "To improve upon the ML workflows we built in this quickstart, you could, for instance:\n",
    "- [Deploy ZenML on the Cloud]() to collaborate with your teammates,\n",
    "- Experiment with more sophisticated models, such as [XGBoost](https://zenml.io/integrations/xgboost),\n",
    "- Set up automated [Slack alerts](https://zenml.io/integrations/zen-ml-slack-integration) to get notified when data drift happens,\n",
    "- Run the pipelines on scalable, distributed stacks like [Kubeflow](https://zenml.io/integrations/kubeflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go next\n",
    "\n",
    "* If you have questions or feedback... \n",
    "  * Join our [**Slack Community**](https://zenml.io/slack-invite) and become part of the ZenML family!\n",
    "* If this quickstart was a bit too quick for you... \n",
    "  * Check out [**ZenBytes**](https://github.com/zenml-io/zenbytes), our lesson series on practical MLOps, where we cover each MLOps concept in much more detail.\n",
    "* If you want to learn more about using or extending ZenML...\n",
    "  * Check out our [**Docs**](https://docs.zenml.io/) or read through our code on [**Github**](https://github.com/zenml-io/zenml).\n",
    "* If you want to quickly learn how to use a specific tool with ZenML...\n",
    "  * Check out our collection of [**Examples**](https://github.com/zenml-io/zenml/tree/doc/hamza-misc-updates/examples).\n",
    "* If you want to see some advanced ZenML use cases... \n",
    "  * Check out [**ZenML Projects**](https://github.com/zenml-io/zenml-projects), our collection of production-grade ML use-cases."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ZenML Quickstart.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d2a2855dd99d151bf9fdc91431d39c1e05805b2488b8cd3e8da8d54747db678"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
